#!/usr/bin/env python3
"""
SARIF æ–‡ä»¶è§£æè„šæœ¬
å°† results.sarif è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„ JSON æ–‡ä»¶
"""

import json
import os
from datetime import datetime

def parse_sarif_file(sarif_file_path, output_file_path):
    """
    è§£æ SARIF æ–‡ä»¶å¹¶ç”Ÿæˆæ ¼å¼åŒ–çš„ JSON è¾“å‡º
    """
    try:
        # è¯»å– SARIF æ–‡ä»¶
        with open(sarif_file_path, 'r', encoding='utf-8') as f:
            sarif_data = json.load(f)
        
        # æå–å…³é”®ä¿¡æ¯
        parsed_results = {
            "metadata": {
                "source_file": sarif_file_path,
                "parsed_at": datetime.now().isoformat(),
                "total_runs": len(sarif_data.get("runs", [])),
                "schema": sarif_data.get("$schema", "")
            },
            "results": []
        }
        
        # éå†æ‰€æœ‰ runs
        for run_index, run in enumerate(sarif_data.get("runs", [])):
            run_info = {
                "run_index": run_index,
                "tool": run.get("tool", {}).get("driver", {}).get("name", "Unknown"),
                "version": run.get("tool", {}).get("driver", {}).get("version", "Unknown"),
                "total_results": len(run.get("results", [])),
                "results": []
            }
            
            # éå†æ‰€æœ‰ç»“æœ
            for result_index, result in enumerate(run.get("results", [])):
                result_info = {
                    "result_index": result_index,
                    "rule_id": result.get("ruleId", ""),
                    "level": result.get("level", "warning"),
                    "message": result.get("message", {}).get("text", ""),
                    "locations": [],
                    "data_flow": []
                }
                
                # æå–ä½ç½®ä¿¡æ¯
                for location in result.get("locations", []):
                    physical_location = location.get("physicalLocation", {})
                    artifact_location = physical_location.get("artifactLocation", {})
                    region = physical_location.get("region", {})
                    
                    location_info = {
                        "file": artifact_location.get("uri", ""),
                        "start_line": region.get("startLine", 0),
                        "start_column": region.get("startColumn", 0),
                        "end_line": region.get("endLine", 0),
                        "end_column": region.get("endColumn", 0)
                    }
                    result_info["locations"].append(location_info)
                
                # æå–æ•°æ®æµä¿¡æ¯
                for code_flow in result.get("codeFlows", []):
                    for thread_flow in code_flow.get("threadFlows", []):
                        flow_steps = []
                        files_involved = set()  # ç”¨äºæ£€æµ‹è·¨æ–‡ä»¶æµ
                        
                        for step in thread_flow.get("locations", []):
                            step_location = step.get("location", {}).get("physicalLocation", {})
                            step_artifact = step_location.get("artifactLocation", {})
                            step_region = step_location.get("region", {})
                            
                            flow_step = {
                                "step_number": step.get("index", 0),
                                "node_type": step.get("nodeType", ""),
                                "description": step.get("description", {}).get("text", ""),
                                "file": step_artifact.get("uri", ""),
                                "line": step_region.get("startLine", 0),
                                "column": step_region.get("startColumn", 0)
                            }
                            
                            flow_steps.append(flow_step)
                            files_involved.add(step_artifact.get("uri", ""))  # è®°å½•æ¶‰åŠçš„æ–‡ä»¶
                        
                        # å¦‚æœæ¶‰åŠå¤šä¸ªæ–‡ä»¶ï¼Œæ ‡è®°ä¸ºè·¨æ–‡ä»¶æµ
                        if len(files_involved) > 1:
                            result_info["data_flow"].append({
                                "thread_flow_index": len(result_info["data_flow"]),
                                "steps": flow_steps,
                                "is_cross_file_flow": True
                            })
                        elif flow_steps:
                            result_info["data_flow"].append({
                                "thread_flow_index": len(result_info["data_flow"]),
                                "steps": flow_steps,
                                "is_cross_file_flow": False
                            })
                
                if result_info["data_flow"]:
                    run_info["results"].append(result_info)
            
            parsed_results["results"].append(run_info)
        
        # ä¿å­˜æ ¼å¼åŒ–çš„ JSON
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(parsed_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… SARIF æ–‡ä»¶è§£æå®Œæˆï¼")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {sarif_file_path}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file_path}")
        print(f"ğŸ“Š æ€»è¿è¡Œæ•°: {parsed_results['metadata']['total_runs']}")
        
        total_findings = sum(run['total_results'] for run in parsed_results['results'])
        print(f"ğŸ” æ€»å‘ç°æ•°: {total_findings}")
        
        return parsed_results
        
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {sarif_file_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSON è§£æå¤±è´¥ - {e}")
        return None
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return None

def create_summary(parsed_data, summary_file_path):
    """
    åˆ›å»ºæ‘˜è¦æŠ¥å‘Š
    """
    if not parsed_data:
        return
    
    summary = {
        "summary": {
            "total_runs": parsed_data["metadata"]["total_runs"],
            "total_findings": 0,
            "findings_by_level": {},
            "findings_by_rule": {}
        },
        "detailed_findings": []
    }
    
    for run in parsed_data["results"]:
        summary["summary"]["total_findings"] += run["total_results"]
        
        for result in run["results"]:
            # æŒ‰çº§åˆ«ç»Ÿè®¡
            level = result["level"]
            summary["summary"]["findings_by_level"][level] = summary["summary"]["findings_by_level"].get(level, 0) + 1
            
            # æŒ‰è§„åˆ™ç»Ÿè®¡
            rule_id = result["rule_id"]
            summary["summary"]["findings_by_rule"][rule_id] = summary["summary"]["findings_by_rule"].get(rule_id, 0) + 1
            
            # è¯¦ç»†å‘ç°
            finding = {
                "rule_id": rule_id,
                "level": level,
                "message": result["message"],
                "locations": result["locations"],
                "data_flow_steps": len(result.get("data_flow", [{}])[0].get("steps", [])) if result.get("data_flow") else 0
            }
            summary["detailed_findings"].append(finding)
    
    # ä¿å­˜æ‘˜è¦
    with open(summary_file_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {summary_file_path}")

if __name__ == "__main__":
    # æ–‡ä»¶è·¯å¾„
    sarif_file = "./test/results.sarif"
    output_file = "./test/parsed_results.json"
    summary_file = "./test/summary_report.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(sarif_file):
        print(f"âŒ é”™è¯¯: å½“å‰ç›®å½•ä¸‹æ‰¾ä¸åˆ° {sarif_file}")
        print("è¯·ç¡®ä¿ results.sarif æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•")
        exit(1)
    
    # è§£æ SARIF æ–‡ä»¶
    parsed_data = parse_sarif_file(sarif_file, output_file)
    
    # åˆ›å»ºæ‘˜è¦æŠ¥å‘Š
    if parsed_data:
        create_summary(parsed_data, summary_file)
        
        print("\nğŸ‰ è§£æå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   ğŸ“„ è¯¦ç»†ç»“æœ: {output_file}")
        print(f"   ğŸ“Š æ‘˜è¦æŠ¥å‘Š: {summary_file}")
        print(f"\nğŸ’¡ ä½¿ç”¨ 'cat {output_file} | jq .' æŸ¥çœ‹æ ¼å¼åŒ–ç»“æœ")
